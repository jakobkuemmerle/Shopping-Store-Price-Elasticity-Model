{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78f05b5",
   "metadata": {},
   "source": [
    "### Functions for cleaning + Feature selection + Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b30dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_transaction_data(trans_file_path):\n",
    "    trans = pd.read_csv(trans_file_path, names=['sku', 'storeid', 'register', 'trannum', 'interID', 'saledate', 'stype', 'quantity',\n",
    "                                                'orgprice', 'amt', 'seq', 'mic', 'unknown'])\n",
    "    \n",
    "    # Keep only purchases and positive amounts\n",
    "    trans = trans[(trans['stype'] == 'P') & (trans['amt'] > 1) & (trans['orgprice'] > 1)]\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    trans = trans.drop(columns=['interID', 'stype', 'mic', 'unknown'])\n",
    "    \n",
    "    return trans\n",
    "\n",
    "def load_sku_data(skst_file_path):\n",
    "    skst = pd.read_csv(skst_file_path, names=['sku', 'storeid', 'cost', 'retail', 'unknown'])\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    skst = skst.drop(columns=['unknown'])\n",
    "    \n",
    "    # Get non-zero mean retail for each SKU\n",
    "    mean_retail = skst.groupby('sku')['retail'].mean().replace(0, np.nan).fillna(0)\n",
    "    \n",
    "    return skst, mean_retail\n",
    "\n",
    "def merge_dataframes(trans, skst, mean_retail):\n",
    "    trans = pd.merge(trans, skst, on=['sku', 'storeid'], how='left')\n",
    "    \n",
    "    # Fill NaN values in 'retail' and 'orgprice' with appropriate values\n",
    "    trans['retail'] = trans['retail'].fillna(trans['sku'].map(mean_retail)).fillna(trans['orgprice'])\n",
    "    trans['orgprice'] = trans['orgprice'].fillna(trans['sku'].map(mean_retail))\n",
    "    \n",
    "    return trans\n",
    "\n",
    "def feature_engineering(trans):\n",
    "    trans['saledate'] = pd.to_datetime(trans['saledate'])\n",
    "    trans['day_of_week'] = trans['saledate'].dt.dayofweek\n",
    "    trans['month'] = trans['saledate'].dt.month\n",
    "    trans['weekend'] = trans['day_of_week'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    trans['amt'] = np.where(trans['amt'] == 0, trans['retail'], trans['amt'])\n",
    "    trans['amt'] = np.where(trans['amt'] == 0, trans['orgprice'], trans['amt'])\n",
    "    \n",
    "    trans['percent_discount'] = np.maximum(0, (trans['orgprice'] - trans['amt']) / trans['orgprice'])\n",
    "    trans.loc[trans['amt'] >= trans['orgprice'], 'percent_discount'] = 0\n",
    "    trans.loc[trans['orgprice'] <= 0, 'percent_discount'] = 0\n",
    "    trans.loc[trans['percent_discount'] < 0, 'percent_discount'] = 0\n",
    "    \n",
    "    trans['final_sale'] = np.where(trans['percent_discount'] > 0.5, 1, 0)\n",
    "    \n",
    "    return trans\n",
    "\n",
    "def join_dataframes(input_df, csv_file_path, columns, join_key='sku', how='inner'):\n",
    "    columns += [join_key]\n",
    "    csv_df = pd.read_csv(csv_file_path)\n",
    "    csv_df.columns = ['sku', 'deptid', 'classid', 'upc', 'style', 'color', 'size', 'packsize', 'vendor', 'brand']\n",
    "    \n",
    "    joined_df = pd.merge(input_df, csv_df[columns], on=join_key, how=how)\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def get_high_value_df(joined_df, n=50):\n",
    "    sku_sum_revenue = joined_df.groupby('sku')['amt'].sum()\n",
    "    sorted_skus = sku_sum_revenue.sort_values(ascending=False).reset_index()\n",
    "    sorted_skus['cumulative_sum'] = sorted_skus['amt'].cumsum()\n",
    "    \n",
    "    max_revenue = sorted_skus['cumulative_sum'].max() / (100 / n)\n",
    "    high_value_skus = sorted_skus[sorted_skus.cumulative_sum < max_revenue]\n",
    "    high_value_df = joined_df[joined_df['sku'].isin(high_value_skus['sku'])]\n",
    "    \n",
    "    return high_value_df\n",
    "\n",
    "def filter_min_average_discount(high_value_df, percentage_column='percent_discount', min_average_discount=0.03):\n",
    "    average_discount = high_value_df.groupby('sku')[percentage_column].mean().reset_index(name='avg_discount')\n",
    "    filtered_df = average_discount[average_discount['avg_discount'] >= min_average_discount]\n",
    "    merged_df = pd.merge(high_value_df, filtered_df, on='sku', how='inner')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28e3b6",
   "metadata": {},
   "source": [
    "### Pipeline to execute all Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d86ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(trans_file_path, skst_file_path, clean_sku_file_path):\n",
    "    trans = load_transaction_data(trans_file_path)\n",
    "    skst, mean_retail = load_sku_data(skst_file_path)\n",
    "    trans = merge_dataframes(trans, skst, mean_retail)\n",
    "    trans = feature_engineering(trans)\n",
    "    \n",
    "    columns = ['brand', 'classid']\n",
    "    joined_df = join_dataframes(trans, clean_sku_file_path, columns)\n",
    "    \n",
    "    high_value_df = get_high_value_df(joined_df)\n",
    "    \n",
    "    filtered_df = filter_min_average_discount(high_value_df)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3cae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basket_data(trans_file_path, skst_file_path, clean_sku_file_path, num_baskets):\n",
    "    trans = load_transaction_data(trans_file_path)\n",
    "    skst, mean_retail = load_sku_data(skst_file_path)\n",
    "    trans = merge_dataframes(trans, skst, mean_retail)\n",
    "    trans = feature_engineering(trans)\n",
    "    \n",
    "    columns = ['brand', 'classid']\n",
    "    joined_df = join_dataframes(trans, clean_sku_file_path, columns)\n",
    "    \n",
    "    high_value_df = get_high_value_df(joined_df)\n",
    "    \n",
    "    filtered_df = filter_min_average_discount(high_value_df)\n",
    "    \n",
    "    baskets = filtered_df.copy()\n",
    "    baskets['sku'] = baskets['sku'].astype(str)\n",
    "    \n",
    "    baskets = baskets.groupby(['saledate', 'storeid', 'register', 'trannum'])['sku'].agg(['count', 'nunique', list]).reset_index()\n",
    "    baskets.columns = ['saledate', 'storeid', 'register', 'trannum', 'TotalItems', 'UniqueItems', 'Items']\n",
    "    \n",
    "    baskets = baskets[(baskets['Items'].apply(len) > 1) & (baskets['UniqueItems'] > 1)].head(num_baskets)\n",
    "    \n",
    "    return baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c69dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run all functions with one click\n",
    "directory = 'Dillards POS/'\n",
    "skst_file_path = directory + 'skstinfo.csv'\n",
    "trans_file_path = directory + 'trans_final.csv'\n",
    "clean_sku_file_path = directory + 'sku_final.csv'\n",
    "\n",
    "baskets_final = process_basket_data(trans_file_path, skst_file_path, clean_sku_file_path, 100000)\n",
    "baskets_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_final = process_data(trans_file_path, skst_file_path, clean_sku_file_path)\n",
    "trans_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trans_final['amt'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c89fd9",
   "metadata": {},
   "source": [
    "## Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cdfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a one-hot encoded format\n",
    "basket_encoded = baskets_final.set_index(['saledate', 'storeid', 'register', 'trannum'])['Items'].apply(pd.Series).stack().reset_index().groupby(['saledate', 'storeid', 'register', 'trannum', 0]).size().unstack().reset_index().fillna(0).set_index(['saledate', 'storeid', 'register', 'trannum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f07cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          antecedents       consequents  antecedent support  \\\n",
      "0           (1563503)         (1453503)             0.00174   \n",
      "1           (1453503)         (1563503)             0.00133   \n",
      "2           (1563503)         (1543503)             0.00174   \n",
      "3           (1543503)         (1563503)             0.00142   \n",
      "4           (1761637)         (1801637)             0.00178   \n",
      "..                ...               ...                 ...   \n",
      "489  (878635, 858635)          (888635)             0.00142   \n",
      "490  (888635, 858635)          (878635)             0.00133   \n",
      "491          (878635)  (888635, 858635)             0.00226   \n",
      "492          (888635)  (878635, 858635)             0.00230   \n",
      "493          (858635)  (878635, 888635)             0.00202   \n",
      "\n",
      "     consequent support  support  confidence        lift  leverage  \\\n",
      "0               0.00133  0.00100    0.574713  432.114770  0.000998   \n",
      "1               0.00174  0.00100    0.751880  432.114770  0.000998   \n",
      "2               0.00142  0.00113    0.649425  457.341752  0.001128   \n",
      "3               0.00174  0.00113    0.795775  457.341752  0.001128   \n",
      "4               0.00162  0.00129    0.724719  447.357470  0.001287   \n",
      "..                  ...      ...         ...         ...       ...   \n",
      "489             0.00230  0.00110    0.774648  336.803429  0.001097   \n",
      "490             0.00226  0.00110    0.827068  365.959146  0.001097   \n",
      "491             0.00133  0.00110    0.486726  365.959146  0.001097   \n",
      "492             0.00142  0.00110    0.478261  336.803429  0.001097   \n",
      "493             0.00172  0.00110    0.544554  316.601428  0.001097   \n",
      "\n",
      "     conviction  zhangs_metric  \n",
      "0      2.348224       0.999425  \n",
      "1      4.023290       0.999014  \n",
      "2      2.848409       0.999553  \n",
      "3      4.888032       0.999232  \n",
      "4      3.626768       0.999544  \n",
      "..          ...            ...  \n",
      "489    4.427294       0.998449  \n",
      "490    5.769540       0.998596  \n",
      "491    1.945685       0.999526  \n",
      "492    1.913945       0.999329  \n",
      "493    2.191876       0.998859  \n",
      "\n",
      "[494 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary representation to boolean for Apriori algorithm\n",
    "basket_sets = basket_encoded.astype(bool)\n",
    "\n",
    "# Use Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.001, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "\n",
    "# Display the rules\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133fa4e",
   "metadata": {},
   "source": [
    "### Explore Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f61aa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>(6276633)</td>\n",
       "      <td>(6756633)</td>\n",
       "      <td>0.00196</td>\n",
       "      <td>0.00212</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>454.851752</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>27.940640</td>\n",
       "      <td>0.999761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>(8018679)</td>\n",
       "      <td>(8188679)</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>485.542826</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>18.763340</td>\n",
       "      <td>0.999820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>(6340353, 6320353)</td>\n",
       "      <td>(6300353)</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>0.918182</td>\n",
       "      <td>419.261104</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>12.195456</td>\n",
       "      <td>0.998713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>(8188679)</td>\n",
       "      <td>(8018679)</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>485.542826</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>11.449024</td>\n",
       "      <td>0.999890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>(6520353, 6510353)</td>\n",
       "      <td>(6500353)</td>\n",
       "      <td>0.00116</td>\n",
       "      <td>0.00229</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>395.271796</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>10.521305</td>\n",
       "      <td>0.998629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            antecedents consequents  antecedent support  consequent support  \\\n",
       "196           (6276633)   (6756633)             0.00196             0.00212   \n",
       "298           (8018679)   (8188679)             0.00188             0.00195   \n",
       "411  (6340353, 6320353)   (6300353)             0.00110             0.00219   \n",
       "299           (8188679)   (8018679)             0.00195             0.00188   \n",
       "428  (6520353, 6510353)   (6500353)             0.00116             0.00229   \n",
       "\n",
       "     support  confidence        lift  leverage  conviction  zhangs_metric  \n",
       "196  0.00189    0.964286  454.851752  0.001886   27.940640       0.999761  \n",
       "298  0.00178    0.946809  485.542826  0.001776   18.763340       0.999820  \n",
       "411  0.00101    0.918182  419.261104  0.001008   12.195456       0.998713  \n",
       "299  0.00178    0.912821  485.542826  0.001776   11.449024       0.999890  \n",
       "428  0.00105    0.905172  395.271796  0.001047   10.521305       0.998629  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.sort_values('confidence', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cbbb2",
   "metadata": {},
   "source": [
    "### Check if Rules seem plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
